// ========================================
// DBF SERVICE (CONSOLIDATED)
// ========================================

import * as fs from 'fs-extra';
import * as path from 'path';
import * as iconv from 'iconv-lite';
import { PrismaClient } from '@prisma/client';
import { FileProcessingStatus, DBFField, DBFHeader, DBFRecord, DBFParseResult } from '@/types';
import { logInfo, logError } from '@/utils/logger';

// ========================================
// INTERFACES
// ========================================

// Interfaces moved to @/types

// ========================================
// DBF READER CLASS
// ========================================

export class DBFReader {
  private buffer: Buffer;
  private fields: DBFField[];
  private headerLength: number;
  private recordLength: number;

  constructor(buffer: Buffer) {
    this.buffer = buffer;
    this.fields = this.parseFields();
    this.headerLength = 32 + (this.fields.length * 32) + 1;
    this.recordLength = this.fields.reduce((sum, field) => sum + field.length, 1);
  }

  private parseFields(): DBFField[] {
    const fields: DBFField[] = [];
    let offset = 32; // Skip file header

    while (this.buffer[offset] !== 0x0D) {
      const fieldName = this.buffer.toString('ascii', offset, offset + 11).replace(/\0/g, '');
      const fieldType = String.fromCharCode(this.buffer[offset + 11] || 0);
      const fieldLength = this.buffer[offset + 16] || 0;
      const decimalPlaces = this.buffer[offset + 17] || 0;

      fields.push({
        name: fieldName,
        type: fieldType,
        length: fieldLength,
        decimalPlaces: decimalPlaces
      });

      offset += 32;
    }

    return fields;
  }

  public getFields(): DBFField[] {
    return this.fields;
  }

  /**
   * ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• records ‡πÅ‡∏ö‡∏ö streaming ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory
   */
  public async processRecordsWithStreaming(
    onRecord: (record: DBFRecord, index: number) => Promise<void>,
    batchSize: number = 1000
  ): Promise<number> {
    let offset = this.headerLength;
    let recordIndex = 0;
    let processedCount = 0;

    while (offset < this.buffer.length) {
      if (this.buffer[offset] === 0x1A) break; // End of file marker

      const record = this.parseSingleRecord(offset);
      if (record) {
        await onRecord(record, recordIndex);
        processedCount++;
      }

      recordIndex++;
      offset += this.recordLength;

      // Force garbage collection ‡∏ó‡∏∏‡∏Å batch
      if (recordIndex % batchSize === 0) {
        if ((global as any).gc) {
          (global as any).gc();
        }
      }
    }

    return processedCount;
  }

  private parseSingleRecord(offset: number): DBFRecord | null {
    if (this.buffer[offset] === 0x1A) return null; // End of file marker
    // dBase/deleted flag: 0x2A ('*') = deleted, 0x20 (' ') = active
    if (this.buffer[offset] === 0x2A) return null; // Deleted record marker

    const record: DBFRecord = {};
    let fieldOffset = offset + 1; // Skip deletion flag

    for (const field of this.fields) {
      const fieldData = this.buffer.slice(fieldOffset, fieldOffset + field.length);
      let value: any = null;

      switch (field.type) {
        case 'C': // Character
          value = iconv.decode(fieldData, 'tis-620').trim();
          break;
        case 'N': // Numeric
          const numStr = fieldData.toString('ascii').trim();
          if (numStr) {
            value = field.decimalPlaces > 0 ? parseFloat(numStr) : parseInt(numStr, 10);
          }
          break;
        case 'D': // Date
          const dateStr = fieldData.toString('ascii').trim();
          value = this.parseDate(dateStr);
          break;
        case 'L': // Logical
          const logicalStr = fieldData.toString('ascii').trim().toUpperCase();
          value = logicalStr === 'T' || logicalStr === 'Y';
          break;
        case 'M': // Memo
          value = fieldData.toString('ascii').trim();
          break;
        default:
          value = fieldData.toString('ascii').trim();
      }

      record[field.name] = value;
      fieldOffset += field.length;
    }

    return record;
  }

  public parseRecords(): DBFRecord[] {
    const records: DBFRecord[] = [];
    let offset = this.headerLength;

    while (offset < this.buffer.length) {
      if (this.buffer[offset] === 0x1A) break; // End of file marker

      const record = this.parseSingleRecord(offset);
      if (record) {
        records.push(record);
      }

      offset += this.recordLength;
    }

    return records;
  }

  private parseDate(dateStr: string): Date | null {
    if (!dateStr || dateStr.trim() === '') return null;

    try {
      // ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö YYYYMMDD
      const year = parseInt(dateStr.substring(0, 4));
      const month = parseInt(dateStr.substring(4, 6)) - 1; // Month is 0-based
      const day = parseInt(dateStr.substring(6, 8));

      if (year < 1900 || year > 2100) return null;
      if (month < 0 || month > 11) return null;
      if (day < 1 || day > 31) return null;

      return new Date(year, month, day);
    } catch (e) {
      return null;
    }
  }

  public getRecordCount(): number {
    // ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô records ‡∏à‡∏≤‡∏Å header
    const recordCountBytes = this.buffer.slice(4, 8);
    return recordCountBytes.readUInt32LE(0);
  }

  public getHeaderInfo(): DBFHeader {
    const recordCount = this.getRecordCount();
    const year = (this.buffer[1] || 0) + 1900;
    const month = this.buffer[2] || 0;
    const day = this.buffer[3] || 0;
    const version = this.buffer[0] || 0;

    return {
      version,
      year,
      month,
      day,
      recordCount,
      headerLength: this.headerLength,
      recordLength: this.recordLength,
      fields: this.fields
    };
  }
}

// ========================================
// DBF SERVICE CLASS
// ========================================

export class DBFService {
  private prisma: PrismaClient;

  constructor(prisma: PrismaClient) {
    this.prisma = prisma;
  }

  /**
   * ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå DBF
   */
  async parseDBFFile(filePath: string): Promise<DBFParseResult> {
    try {
      logInfo(`üîç ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå DBF: ${path.basename(filePath)}`);

      // ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πá‡∏ô Buffer
      const buffer = await fs.readFile(filePath);

      // ‡πÉ‡∏ä‡πâ DBFReader class
      const reader = new DBFReader(buffer);
      const header = reader.getHeaderInfo();
      const records = reader.parseRecords();

      logInfo(`‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå DBF ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: ${header.recordCount} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£, ${header.fields.length} ‡∏ü‡∏¥‡∏•‡∏î‡πå`);

      return {
        header,
        records,
        schema: header.fields
      };
    } catch (error) {
      logError('Error parsing DBF file', error as Error);
      throw new Error(`‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå DBF ‡πÑ‡∏î‡πâ: ${(error as Error).message}`);
    }
  }

  /**
   * ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå DBF ‡πÅ‡∏ö‡∏ö streaming
   */
  async processDBFWithStreaming(
    filePath: string,
    onRecord: (record: DBFRecord, index: number) => Promise<void>,
    batchSize: number = 1000
  ): Promise<{ recordCount: number; fieldCount: number }> {
    try {
      logInfo(`üîç ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå DBF ‡πÅ‡∏ö‡∏ö streaming: ${path.basename(filePath)}`);

      const buffer = await fs.readFile(filePath);
      const reader = new DBFReader(buffer);

      const recordCount = await reader.processRecordsWithStreaming(onRecord, batchSize);
      const fieldCount = reader.getFields().length;

      logInfo(`‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå DBF ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: ${recordCount} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£, ${fieldCount} ‡∏ü‡∏¥‡∏•‡∏î‡πå`);

      return { recordCount, fieldCount };
    } catch (error) {
      logError('Error processing DBF file streaming', error as Error);
      throw new Error(`‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå DBF ‡πÑ‡∏î‡πâ: ${(error as Error).message}`);
    }
  }

  /**
   * ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBF ‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
   */
  async saveDBFRecordsToDatabase(
    _batchId: string,
    records: DBFRecord[],
    fileId: string
  ): Promise<{ savedCount: number; errorCount: number }> {
    try {
      logInfo(`üíæ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBF ‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ${records.length} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£`);

      let savedCount = 0;
      let errorCount = 0;

      for (const record of records) {
        try {
          await this.prisma.dBF_Record.create({
            data: {
              fileId,
              recordIndex: savedCount,
              data: JSON.stringify(record),
              createdAt: new Date()
            }
          });
          savedCount++;
        } catch (error) {
          logError('Error saving DBF record', error as Error);
          errorCount++;
        }
      }

      logInfo(`‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBF ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: ${savedCount} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£, ${errorCount} ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î`);

      return { savedCount, errorCount };
    } catch (error) {
      logError('Error saving DBF records to database', error as Error);
      throw new Error(`‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBF ‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ: ${(error as Error).message}`);
    }
  }

  /**
   * ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå
   */
  async updateFileProcessingStatus(
    fileId: string,
    status: FileProcessingStatus,
    errorMessage?: string
  ): Promise<void> {
    try {
      await this.prisma.uploadRecord.update({
        where: { id: fileId },
        data: {
          status,
          errorMessage: errorMessage || null,
          updatedAt: new Date()
        }
      });

      logInfo(`üìù ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÑ‡∏ü‡∏•‡πå: ${fileId} -> ${status}`);
    } catch (error) {
      logError('Error updating file processing status', error as Error);
      throw new Error(`‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ: ${(error as Error).message}`);
    }
  }

  /**
   * ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBF records ‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OPD
   */
  public async getAllDBFRecordsFromDatabaseForOPD(fileId: string): Promise<any[]> {
    try {
      logInfo('Fetching all DBF records from database for OPD', { fileId });
  
      const prisma = new PrismaClient();
  
      // ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á dbf_records ‡∏ï‡∏≤‡∏° fileId
      const dbfRecords = await prisma.dBF_Record.findMany({
        where: {
          fileId: fileId
        },
        orderBy: {
          id: 'asc'
        }
      });
  
      const records: DBFRecord[] = dbfRecords.map(dbfRecord =>
        JSON.parse(dbfRecord.data_ipd || dbfRecord.data)
      );
  
      await prisma.$disconnect();
  
      logInfo('DBF records fetched successfully', {
        fileId,
        recordCount: records.length
      });
  
      return records;
  
    } catch (error) {
      logError('Failed to fetch DBF records from database', error as Error, { fileId });
      throw error;
    }
  }

  /**
   * ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBF records ‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö IPD
   */
  public async getAllDBFRecordsFromDatabaseForIPD(fileId: string): Promise<any[]> {
    try {
      logInfo('Fetching all DBF records from database for IPD', { fileId });
  
      const prisma = new PrismaClient();
  
      // ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á dbf_records ‡∏ï‡∏≤‡∏° fileId
      const dbfRecords = await prisma.dBF_Record.findMany({
        where: {
          fileId: fileId
        },
        orderBy: {
          id: 'asc'
        }
      });
  
      // ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô objects
      const records: DBFRecord[] = dbfRecords.map(dbfRecord =>
        JSON.parse(dbfRecord.data_ipd || dbfRecord.data)
      );
  
      await prisma.$disconnect();
  
      logInfo('DBF records fetched successfully', {
        fileId,
        recordCount: records.length
      });
  
      return records;
  
    } catch (error) {
      logError('Failed to fetch DBF records from database', error as Error, { fileId });
      throw error;
    }
  }

  /**
   * ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå DBF ‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (instance method)
   */
  public async createDBFFileFromRecords(
    records: DBFRecord[],
    outputPath: string,
    _originalFileName: string
  ): Promise<void> {
    try {
      logInfo(`üìù ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå DBF: ${path.basename(outputPath)}`);
  
      // ‡∏™‡∏£‡πâ‡∏≤‡∏á schema ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏£‡∏Å (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
      let schema: DBFField[] = [];
      if (records.length > 0) {
        const firstRecord = records[0];
        if (firstRecord) {
          schema = Object.keys(firstRecord).map(fieldName => {
            const value = firstRecord[fieldName];
            const valueStr = String(value || '');
  
            return {
              name: fieldName,
              type: 'C', // Character type ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
              length: Math.max(valueStr.length, 10), // ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥ 10
              decimalPlaces: 0
            };
          });
        }
      }
  
      // ‡∏™‡∏£‡πâ‡∏≤‡∏á header
      const header = createDBFHeader(records.length, schema);
  
      // ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå DBF
      const buffer = createDBFBuffer(header, records, schema);
  
      // ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå
      await fs.writeFile(outputPath, buffer);
  
      logInfo(`‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå DBF ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: ${path.basename(outputPath)} (${records.length} records)`);
  
    } catch (error) {
      logError('Error creating DBF file from records', error as Error);
      throw new Error(`‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå DBF ‡πÑ‡∏î‡πâ: ${(error as Error).message}`);
    }
  }
}

// ========================================
// UTILITY FUNCTIONS
// ========================================

export function parseDBFWithSchema(buffer: Buffer): {
  records: DBFRecord[];
  schema: DBFField[];
} {
  const reader = new DBFReader(buffer);
  const records = reader.parseRecords();
  const schema = reader.getFields();

  return { records, schema };
}

export function isADPFile(filename: string, fields: DBFField[]): boolean {
  const filenameUpper = filename.toUpperCase();
  const hasADPFields = fields.some(field =>
    field.name.toUpperCase() === 'CODE' ||
    field.name.toUpperCase() === 'QTY' ||
    field.name.toUpperCase() === 'RATE' ||
    field.name.toUpperCase() === 'TOTAL'
  );

  return filenameUpper.includes('ADP') || hasADPFields;
}

export function isOPDFile(filename: string): boolean {
  const filenameUpper = filename.toUpperCase();
  return filenameUpper.includes('OPD') || filenameUpper.includes('PAT');
}

export function isCHTFile(filename: string): boolean {
  const filenameUpper = filename.toUpperCase();
  return filenameUpper.includes('CHT');
}

export function isCHAFile(filename: string): boolean {
  const filenameUpper = filename.toUpperCase();
  return filenameUpper.includes('CHA');
}

export function isINSFile(filename: string): boolean {
  const filenameUpper = filename.toUpperCase();
  return filenameUpper.includes('INS');
}

export function isDRUFile(filename: string): boolean {
  const filenameUpper = filename.toUpperCase();
  return filenameUpper.includes('DRU');
}

export function isODXFile(filename: string): boolean {
  const filenameUpper = filename.toUpperCase();
  return filenameUpper.includes('ODX');
}

/**
   * ‡∏™‡∏£‡πâ‡∏≤‡∏á DBF header
   */
export function createDBFHeader(recordCount: number, fields: DBFField[]): DBFHeader {
  const now = new Date();
  const headerLength = 32 + (fields.length * 32) + 1; // 32 bytes header + field definitions + terminator
  const recordLength = fields.reduce((sum, field) => sum + field.length, 1); // +1 for deletion flag

  return {
    version: 3, // dBASE III
    year: now.getFullYear() - 1900, // DBF ‡πÉ‡∏ä‡πâ‡∏õ‡∏µ 1900 ‡πÄ‡∏õ‡πá‡∏ô‡∏ê‡∏≤‡∏ô
    month: now.getMonth() + 1,
    day: now.getDate(),
    recordCount,
    headerLength,
    recordLength,
    fields
  };
}

/**
  * ‡∏™‡∏£‡πâ‡∏≤‡∏á DBF buffer
  */
export function createDBFBuffer(header: DBFHeader, records: DBFRecord[], fields: DBFField[]): Buffer {
  // ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î buffer
  const headerSize = header.headerLength;
  const recordSize = header.recordLength;
  const totalSize = headerSize + (records.length * recordSize);

  const buffer = Buffer.alloc(totalSize);
  let offset = 0;

  // ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô header
  buffer[offset++] = header.version;
  buffer[offset++] = header.year;
  buffer[offset++] = header.month;
  buffer[offset++] = header.day;
  buffer.writeUInt32LE(header.recordCount, offset);
  offset += 4;
  buffer.writeUInt16LE(header.headerLength, offset);
  offset += 2;
  buffer.writeUInt16LE(header.recordLength, offset);
  offset += 2;

  // ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô reserved bytes (10 bytes)
  for (let i = 0; i < 10; i++) {
    buffer[offset++] = 0;
  }

  // ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô field definitions
  for (const field of fields) {
    // Field name (11 bytes)
    const nameBuffer = Buffer.from(field.name.padEnd(11, '\0'));
    nameBuffer.copy(buffer, offset);
    offset += 11;

    // Field type (1 byte)
    buffer[offset++] = field.type.charCodeAt(0);

    // Reserved (4 bytes)
    for (let i = 0; i < 4; i++) {
      buffer[offset++] = 0;
    }

    // Field length (1 byte)
    buffer[offset++] = field.length;

    // Decimal places (1 byte)
    buffer[offset++] = field.decimalPlaces;

    // Reserved (14 bytes)
    for (let i = 0; i < 14; i++) {
      buffer[offset++] = 0;
    }
  }

  // Header terminator
  buffer[offset++] = 0x0D;

  // ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô records
  for (const record of records) {
    // Deletion flag
    buffer[offset++] = 0x20; // Space = not deleted

    // Record data
    for (const field of fields) {
      const value = record[field.name] || '';
      const valueStr = String(value);
      const paddedValue = valueStr.padEnd(field.length, ' ');
      const valueBuffer = Buffer.from(paddedValue.substring(0, field.length));
      valueBuffer.copy(buffer, offset);
      offset += field.length;
    }
  }

  return buffer;
}

export default DBFService;
